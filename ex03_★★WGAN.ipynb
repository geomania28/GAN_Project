{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNm83e5BN+mTBxgYRVFWF5p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["출처 : https://keras.io/examples/generative/wgan_gp/"],"metadata":{"id":"qF8N7nkksISS"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"],"metadata":{"id":"_CYdmm40i2m2","executionInfo":{"status":"ok","timestamp":1697863787590,"user_tz":-540,"elapsed":2,"user":{"displayName":"김형오","userId":"12580521154359558487"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-GEQJFlgFlr","executionInfo":{"status":"ok","timestamp":1697863801391,"user_tz":-540,"elapsed":1293,"user":{"displayName":"김형오","userId":"12580521154359558487"}},"outputId":"4d9d4014-c7d7-488b-c5ed-54ab4cf8eab1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","29515/29515 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26421880/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","5148/5148 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4422102/4422102 [==============================] - 0s 0us/step\n","Number of examples: 60000\n","Shape of the images in the dataset: (28, 28)\n"]}],"source":["IMG_SHAPE = (28, 28, 1)\n","BATCH_SIZE = 512\n","\n","# Size of the noise vector\n","noise_dim = 128\n","\n","fashion_mnist = keras.datasets.fashion_mnist\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n","print(f\"Number of examples: {len(train_images)}\")\n","print(f\"Shape of the images in the dataset: {train_images.shape[1:]}\")\n","\n","# Reshape each sample to (28, 28, 1) and normalize the pixel values in the [-1, 1] range\n","train_images = train_images.reshape(train_images.shape[0], *IMG_SHAPE).astype(\"float32\")\n","train_images = (train_images - 127.5) / 127.5"]},{"cell_type":"code","source":["def conv_block(\n","    x,\n","    filters,\n","    activation,\n","    kernel_size=(3, 3),\n","    strides=(1, 1),\n","    padding=\"same\",\n","    use_bias=True,\n","    use_bn=False,\n","    use_dropout=False,\n","    drop_value=0.5,\n","):\n","    x = layers.Conv2D(\n","        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n","    )(x)\n","    if use_bn:\n","        x = layers.BatchNormalization()(x)\n","    x = activation(x)\n","    if use_dropout:\n","        x = layers.Dropout(drop_value)(x)\n","    return x\n","\n","\n","def get_discriminator_model():\n","    img_input = layers.Input(shape=IMG_SHAPE)\n","    # Zero pad the input to make the input images size to (32, 32, 1).\n","    x = layers.ZeroPadding2D((2, 2))(img_input)\n","    x = conv_block(\n","        x,\n","        64,\n","        kernel_size=(5, 5),\n","        strides=(2, 2),\n","        use_bn=False,\n","        use_bias=True,\n","        activation=layers.LeakyReLU(0.2),\n","        use_dropout=False,\n","        drop_value=0.3,\n","    )\n","    x = conv_block(\n","        x,\n","        128,\n","        kernel_size=(5, 5),\n","        strides=(2, 2),\n","        use_bn=False,\n","        activation=layers.LeakyReLU(0.2),\n","        use_bias=True,\n","        use_dropout=True,\n","        drop_value=0.3,\n","    )\n","    x = conv_block(\n","        x,\n","        256,\n","        kernel_size=(5, 5),\n","        strides=(2, 2),\n","        use_bn=False,\n","        activation=layers.LeakyReLU(0.2),\n","        use_bias=True,\n","        use_dropout=True,\n","        drop_value=0.3,\n","    )\n","    x = conv_block(\n","        x,\n","        512,\n","        kernel_size=(5, 5),\n","        strides=(2, 2),\n","        use_bn=False,\n","        activation=layers.LeakyReLU(0.2),\n","        use_bias=True,\n","        use_dropout=False,\n","        drop_value=0.3,\n","    )\n","\n","    x = layers.Flatten()(x)\n","    x = layers.Dropout(0.2)(x)\n","    x = layers.Dense(1)(x)\n","\n","    d_model = keras.models.Model(img_input, x, name=\"discriminator\")\n","    return d_model\n","\n","\n","d_model = get_discriminator_model()\n","d_model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0JiCXXpjFuW","executionInfo":{"status":"ok","timestamp":1697863818045,"user_tz":-540,"elapsed":4,"user":{"displayName":"김형오","userId":"12580521154359558487"}},"outputId":"5ba483d7-9345-4005-e7d0-3c25c2017318"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"discriminator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_4 (InputLayer)        [(None, 28, 28, 1)]       0         \n","                                                                 \n"," zero_padding2d_1 (ZeroPadd  (None, 32, 32, 1)         0         \n"," ing2D)                                                          \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 16, 16, 64)        1664      \n","                                                                 \n"," leaky_re_lu_4 (LeakyReLU)   (None, 16, 16, 64)        0         \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 8, 8, 128)         204928    \n","                                                                 \n"," leaky_re_lu_5 (LeakyReLU)   (None, 8, 8, 128)         0         \n","                                                                 \n"," dropout_4 (Dropout)         (None, 8, 8, 128)         0         \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 4, 4, 256)         819456    \n","                                                                 \n"," leaky_re_lu_6 (LeakyReLU)   (None, 4, 4, 256)         0         \n","                                                                 \n"," dropout_5 (Dropout)         (None, 4, 4, 256)         0         \n","                                                                 \n"," conv2d_10 (Conv2D)          (None, 2, 2, 512)         3277312   \n","                                                                 \n"," leaky_re_lu_7 (LeakyReLU)   (None, 2, 2, 512)         0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 2048)              0         \n","                                                                 \n"," dropout_6 (Dropout)         (None, 2048)              0         \n","                                                                 \n"," dense_2 (Dense)             (None, 1)                 2049      \n","                                                                 \n","=================================================================\n","Total params: 4305409 (16.42 MB)\n","Trainable params: 4305409 (16.42 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["def upsample_block(\n","    x,\n","    filters,\n","    activation,\n","    kernel_size=(3, 3),\n","    strides=(1, 1),\n","    up_size=(2, 2),\n","    padding=\"same\",\n","    use_bn=False,\n","    use_bias=True,\n","    use_dropout=False,\n","    drop_value=0.3,\n","):\n","    x = layers.UpSampling2D(up_size)(x)\n","    x = layers.Conv2D(\n","        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n","    )(x)\n","\n","    if use_bn:\n","        x = layers.BatchNormalization()(x)\n","\n","    if activation:\n","        x = activation(x)\n","    if use_dropout:\n","        x = layers.Dropout(drop_value)(x)\n","    return x\n","\n","\n","def get_generator_model():\n","    noise = layers.Input(shape=(noise_dim,))\n","    x = layers.Dense(4 * 4 * 256, use_bias=False)(noise)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU(0.2)(x)\n","\n","    x = layers.Reshape((4, 4, 256))(x)\n","    x = upsample_block(\n","        x,\n","        128,\n","        layers.LeakyReLU(0.2),\n","        strides=(1, 1),\n","        use_bias=False,\n","        use_bn=True,\n","        padding=\"same\",\n","        use_dropout=False,\n","    )\n","    x = upsample_block(\n","        x,\n","        64,\n","        layers.LeakyReLU(0.2),\n","        strides=(1, 1),\n","        use_bias=False,\n","        use_bn=True,\n","        padding=\"same\",\n","        use_dropout=False,\n","    )\n","    x = upsample_block(\n","        x, 1, layers.Activation(\"tanh\"), strides=(1, 1), use_bias=False, use_bn=True\n","    )\n","    # At this point, we have an output which has the same shape as the input, (32, 32, 1).\n","    # We will use a Cropping2D layer to make it (28, 28, 1).\n","    x = layers.Cropping2D((2, 2))(x)\n","\n","    g_model = keras.models.Model(noise, x, name=\"generator\")\n","    return g_model\n","\n","\n","g_model = get_generator_model()\n","g_model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oRzCyX77gIjl","executionInfo":{"status":"ok","timestamp":1697863834089,"user_tz":-540,"elapsed":520,"user":{"displayName":"김형오","userId":"12580521154359558487"}},"outputId":"4b47d924-d1e8-4b04-fe79-034aa46094d3"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"generator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_5 (InputLayer)        [(None, 128)]             0         \n","                                                                 \n"," dense_3 (Dense)             (None, 4096)              524288    \n","                                                                 \n"," batch_normalization_5 (Bat  (None, 4096)              16384     \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_8 (LeakyReLU)   (None, 4096)              0         \n","                                                                 \n"," reshape_1 (Reshape)         (None, 4, 4, 256)         0         \n","                                                                 \n"," up_sampling2d_2 (UpSamplin  (None, 8, 8, 256)         0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 8, 8, 128)         294912    \n","                                                                 \n"," batch_normalization_6 (Bat  (None, 8, 8, 128)         512       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_9 (LeakyReLU)   (None, 8, 8, 128)         0         \n","                                                                 \n"," up_sampling2d_3 (UpSamplin  (None, 16, 16, 128)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_12 (Conv2D)          (None, 16, 16, 64)        73728     \n","                                                                 \n"," batch_normalization_7 (Bat  (None, 16, 16, 64)        256       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_10 (LeakyReLU)  (None, 16, 16, 64)        0         \n","                                                                 \n"," up_sampling2d_4 (UpSamplin  (None, 32, 32, 64)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_13 (Conv2D)          (None, 32, 32, 1)         576       \n","                                                                 \n"," batch_normalization_8 (Bat  (None, 32, 32, 1)         4         \n"," chNormalization)                                                \n","                                                                 \n"," activation_3 (Activation)   (None, 32, 32, 1)         0         \n","                                                                 \n"," cropping2d (Cropping2D)     (None, 28, 28, 1)         0         \n","                                                                 \n","=================================================================\n","Total params: 910660 (3.47 MB)\n","Trainable params: 902082 (3.44 MB)\n","Non-trainable params: 8578 (33.51 KB)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["class WGAN(keras.Model):\n","    def __init__(\n","        self,\n","        discriminator,\n","        generator,\n","        latent_dim,\n","        discriminator_extra_steps=3,\n","        gp_weight=10.0,\n","    ):\n","        super().__init__()\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.latent_dim = latent_dim\n","        self.d_steps = discriminator_extra_steps\n","        self.gp_weight = gp_weight\n","\n","    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n","        super().compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.d_loss_fn = d_loss_fn\n","        self.g_loss_fn = g_loss_fn\n","\n","    def gradient_penalty(self, batch_size, real_images, fake_images):\n","        \"\"\"Calculates the gradient penalty.\n","\n","        This loss is calculated on an interpolated image\n","        and added to the discriminator loss.\n","        \"\"\"\n","        # Get the interpolated image\n","        alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n","        diff = fake_images - real_images\n","        interpolated = real_images + alpha * diff\n","\n","        with tf.GradientTape() as gp_tape:\n","            gp_tape.watch(interpolated)\n","            # 1. Get the discriminator output for this interpolated image.\n","            pred = self.discriminator(interpolated, training=True)\n","\n","        # 2. Calculate the gradients w.r.t to this interpolated image.\n","        grads = gp_tape.gradient(pred, [interpolated])[0]\n","        # 3. Calculate the norm of the gradients.\n","        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n","        gp = tf.reduce_mean((norm - 1.0) ** 2)\n","        return gp\n","\n","    def train_step(self, real_images):\n","        if isinstance(real_images, tuple):\n","            real_images = real_images[0]\n","\n","        # Get the batch size\n","        batch_size = tf.shape(real_images)[0]\n","\n","        # For each batch, we are going to perform the\n","        # following steps as laid out in the original paper:\n","        # 1. Train the generator and get the generator loss\n","        # 2. Train the discriminator and get the discriminator loss\n","        # 3. Calculate the gradient penalty\n","        # 4. Multiply this gradient penalty with a constant weight factor\n","        # 5. Add the gradient penalty to the discriminator loss\n","        # 6. Return the generator and discriminator losses as a loss dictionary\n","\n","        # Train the discriminator first. The original paper recommends training\n","        # the discriminator for `x` more steps (typically 5) as compared to\n","        # one step of the generator. Here we will train it for 3 extra steps\n","        # as compared to 5 to reduce the training time.\n","        for i in range(self.d_steps):\n","            # Get the latent vector\n","            random_latent_vectors = tf.random.normal(\n","                shape=(batch_size, self.latent_dim)\n","            )\n","            with tf.GradientTape() as tape:\n","                # Generate fake images from the latent vector\n","                fake_images = self.generator(random_latent_vectors, training=True)\n","                # Get the logits for the fake images\n","                fake_logits = self.discriminator(fake_images, training=True)\n","                # Get the logits for the real images\n","                real_logits = self.discriminator(real_images, training=True)\n","\n","                # Calculate the discriminator loss using the fake and real image logits\n","                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n","                # Calculate the gradient penalty\n","                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n","                # Add the gradient penalty to the original discriminator loss\n","                d_loss = d_cost + gp * self.gp_weight\n","\n","            # Get the gradients w.r.t the discriminator loss\n","            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n","            # Update the weights of the discriminator using the discriminator optimizer\n","            self.d_optimizer.apply_gradients(\n","                zip(d_gradient, self.discriminator.trainable_variables)\n","            )\n","\n","        # Train the generator\n","        # Get the latent vector\n","        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","        with tf.GradientTape() as tape:\n","            # Generate fake images using the generator\n","            generated_images = self.generator(random_latent_vectors, training=True)\n","            # Get the discriminator logits for fake images\n","            gen_img_logits = self.discriminator(generated_images, training=True)\n","            # Calculate the generator loss\n","            g_loss = self.g_loss_fn(gen_img_logits)\n","\n","        # Get the gradients w.r.t the generator loss\n","        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n","        # Update the weights of the generator using the generator optimizer\n","        self.g_optimizer.apply_gradients(\n","            zip(gen_gradient, self.generator.trainable_variables)\n","        )\n","        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"],"metadata":{"id":"UOSbkcaLkhSM","executionInfo":{"status":"ok","timestamp":1697863848639,"user_tz":-540,"elapsed":359,"user":{"displayName":"김형오","userId":"12580521154359558487"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class GANMonitor(keras.callbacks.Callback):\n","    def __init__(self, num_img=6, latent_dim=128):\n","        self.num_img = num_img\n","        self.latent_dim = latent_dim\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n","        generated_images = self.model.generator(random_latent_vectors)\n","        generated_images = (generated_images * 127.5) + 127.5\n","\n","        for i in range(self.num_img):\n","            img = generated_images[i].numpy()\n","            img = keras.utils.array_to_img(img)\n","            img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))"],"metadata":{"id":"N_vX1OBEkk30","executionInfo":{"status":"ok","timestamp":1697863860452,"user_tz":-540,"elapsed":357,"user":{"displayName":"김형오","userId":"12580521154359558487"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Instantiate the optimizer for both networks\n","# (learning_rate=0.0002, beta_1=0.5 are recommended)\n","generator_optimizer = keras.optimizers.Adam(\n","    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",")\n","discriminator_optimizer = keras.optimizers.Adam(\n","    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",")\n","\n","# Define the loss functions for the discriminator,\n","# which should be (fake_loss - real_loss).\n","# We will add the gradient penalty later to this loss function.\n","def discriminator_loss(real_img, fake_img):\n","    real_loss = tf.reduce_mean(real_img)\n","    fake_loss = tf.reduce_mean(fake_img)\n","    return fake_loss - real_loss\n","\n","\n","# Define the loss functions for the generator.\n","def generator_loss(fake_img):\n","    return -tf.reduce_mean(fake_img)\n","\n","\n","# Set the number of epochs for training.\n","epochs = 20\n","\n","# Instantiate the customer `GANMonitor` Keras callback.\n","cbk = GANMonitor(num_img=3, latent_dim=noise_dim)\n","\n","# Get the wgan model\n","wgan = WGAN(\n","    discriminator=d_model,\n","    generator=g_model,\n","    latent_dim=noise_dim,\n","    discriminator_extra_steps=3,\n",")\n","\n","# Compile the wgan model\n","wgan.compile(\n","    d_optimizer=discriminator_optimizer,\n","    g_optimizer=generator_optimizer,\n","    g_loss_fn=generator_loss,\n","    d_loss_fn=discriminator_loss,\n",")\n","\n","# Start training\n","wgan.fit(train_images, batch_size=BATCH_SIZE, epochs=epochs, callbacks=[cbk])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"obC2BnYMknwj","executionInfo":{"status":"ok","timestamp":1697866995364,"user_tz":-540,"elapsed":3118526,"user":{"displayName":"김형오","userId":"12580521154359558487"}},"outputId":"b54e3b88-b4f4-4695-eb48-d4f83c637534"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","118/118 [==============================] - 172s 1s/step - d_loss: -7.7885 - g_loss: -17.4003\n","Epoch 2/20\n","118/118 [==============================] - 155s 1s/step - d_loss: -6.9857 - g_loss: -15.5121\n","Epoch 3/20\n","118/118 [==============================] - 155s 1s/step - d_loss: -5.9926 - g_loss: -13.8234\n","Epoch 4/20\n","118/118 [==============================] - 155s 1s/step - d_loss: -5.3826 - g_loss: -11.9976\n","Epoch 5/20\n","118/118 [==============================] - 155s 1s/step - d_loss: -4.9757 - g_loss: -11.1016\n","Epoch 6/20\n","118/118 [==============================] - 155s 1s/step - d_loss: -4.6327 - g_loss: -11.1300\n","Epoch 7/20\n","118/118 [==============================] - 156s 1s/step - d_loss: -4.3527 - g_loss: -9.6855\n","Epoch 8/20\n","118/118 [==============================] - 155s 1s/step - d_loss: -4.1321 - g_loss: -9.8432\n","Epoch 9/20\n","118/118 [==============================] - 154s 1s/step - d_loss: -3.8860 - g_loss: -9.6946\n","Epoch 10/20\n","118/118 [==============================] - 155s 1s/step - d_loss: -3.6718 - g_loss: -9.3893\n","Epoch 11/20\n","118/118 [==============================] - 155s 1s/step - d_loss: -3.5365 - g_loss: -9.4166\n","Epoch 12/20\n","118/118 [==============================] - 156s 1s/step - d_loss: -3.3632 - g_loss: -8.1421\n","Epoch 13/20\n","118/118 [==============================] - 156s 1s/step - d_loss: -3.2416 - g_loss: -7.2522\n","Epoch 14/20\n","118/118 [==============================] - 156s 1s/step - d_loss: -3.0867 - g_loss: -7.4467\n","Epoch 15/20\n","118/118 [==============================] - 154s 1s/step - d_loss: -2.9721 - g_loss: -6.6658\n","Epoch 16/20\n","118/118 [==============================] - 155s 1s/step - d_loss: -2.8996 - g_loss: -6.9899\n","Epoch 17/20\n","118/118 [==============================] - 155s 1s/step - d_loss: -2.8339 - g_loss: -5.7783\n","Epoch 18/20\n","118/118 [==============================] - 155s 1s/step - d_loss: -2.7802 - g_loss: -5.5540\n","Epoch 19/20\n","118/118 [==============================] - 155s 1s/step - d_loss: -2.6642 - g_loss: -5.5974\n","Epoch 20/20\n","118/118 [==============================] - 155s 1s/step - d_loss: -2.6626 - g_loss: -5.1433\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7fe94a99ee60>"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["from IPython.display import Image, display\n","\n","display(Image(\"generated_img_0_19.png\"))\n","display(Image(\"generated_img_1_19.png\"))\n","display(Image(\"generated_img_2_19.png\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101},"id":"JhntK1z8kr2L","executionInfo":{"status":"ok","timestamp":1697867001634,"user_tz":-540,"elapsed":327,"user":{"displayName":"김형오","userId":"12580521154359558487"}},"outputId":"51e12614-802d-4a21-83d7-615c38a485ce"},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACpElEQVR4nAXBS4/bVBQA4HPPPX47diZpppPpzLRpqpEKQ8WyQu0GsWWFxJ+EBRIsETtoVdQVKqpUNK06j0ycOA87tmPfx+H7hIPGotAMELsFCyG9HTAgSIFK+AqFFRi9/OHXGc79I9pM1V+ZAmI2ZBAA2Hv+jPL6KXkDzHrfjn6bKWDLgoBZptMXdhsuPioviULxWBb/vK+2YMkxYP0XD72J1xe7/MH5p0IVi+Oh+K9kIM2MX/wo1iOJ/cGZ0T09wms4mL7+ec5kpZHTk/xELtzYtPWsbEPqaXl6e5h3KBDjaVUqGtC2u6zauH37Lh/a/HrUR7KWowvHievxVkdnbrCs7z02pnS+P99nCIEo0uDeI/UZgvx2kc1GRkGevY1WgUtkbXtfNOUi7g7zeeQvh+4nfdYbPEjuOuSW43P20yYxd+nk9MuJk44Vupf1hQIEySLsBonbglussktNc/cw3Z5s+s8FoRC2nB3sKSkT8kIjS6kFiVG3ZkMW4MRmFxgGiJ6CxHqGi5Fejt7MkMDAd7WNF6nPdaYYNhSH0WzTf/rmGlCgH2lPk1eXmtS+aFCtTfukXLV3gozEs+SYOx34GZ5GzVGnMUrk2ExqQJnK5XUVt1D5fekHnpP4w6gtb/bzFoiNvP273G6bnkz8VeeZamuOK3i4v3EZTTV8OemHce8ImsYVbVen9W6hbup3rUJW69+3stkYSJOgIe56Da0XwsYOSAKuPnwT8yzFwgz8VFd06ik6cAdf/wQk0FkW7eKguCrIh5vXZ2nZN3n2OfxFMbHYz//koLqC+5vAn/97Gd+ORa7F5gMDoWR3HuL+5I/3lOnVWiHf9c24yjoEAitUBOWzxVfLV7NuZ4CtjbxAoXC0EBJhIOnR1c40VmrLLB2GUJsa7P+zDHiPNPoTBQAAAABJRU5ErkJggg==\n","text/plain":["<IPython.core.display.Image object>"]},"metadata":{}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACm0lEQVR4nAXBy25bZRAA4Jn559ztOHbs2HFQ46otUlRBQWzKgqorNmzYsmDBghVvw4In6FOwROVWLr0pUq2mJHLT2LVjx8f2OT63f4bvA3YJHUMG4OD7VtQY/HSf2HUCL2Bj2CipBeDjzsf1k16pp69mh083yGKkYhUgI9i89+D5H7vf1iY/OMc7/E9pVRTQEKAfdj+v9IPL6+/2H73LLr7enE9OssoCEwjIvU//LOrVg+GpfHV24g/3v3lymgMJgyCD9/qy2xv9Yi/eeK1mX178W/aTQpBViaHm3gbKX/WCl6tW8yOcZjeOzhMFIuCdKHoa3blutBve7uG+edPk+votAgihspOb1DIWQS+quUHt7sLqewoJiBTS+ObDzsGvI88Zz7Nl3S4mR50v77ddUAbHrs6e9R+Oh1kft6kUncbd9tnxX7FBZQVBuflJI2zLZmdd1GfTzpMvBqO/F6kYEnCa+uPvB3v9Qax7nG15ctQaDueZohKLTdLxyl1NfSddA5bx1X/O1eu8RAAmADLw+NEo3wRuUPcXmTsuohhJAak0VFbxZCrNskpqmkRbV8NZzIooLGoNgGGJg0ymNsG1ufp5dJ6pKhIZdgC0c1tWVdnwbF453jxOK1BEBguAFPXmN8w8Atf1Cyjfh2IVgJhErIK2aIG5x5IXoW82xiAAKJGCAns19crtZpmjWnKiD+tICMqVohDkagfXE98UlWxdnTmsiKqEqCoF7K6qpAojsoKVJr6jYlWZBAHBgajWk0atXjPXZhs3fVBFIFEggMG2uLPnht12/xCChowNIBKykKiyGaW/RVrMlnYLHnW3bERVGVSJ9N3ys4Qv1l5Ybtqd4DxBBARCIksoO93k1q3luh5c7ibp29TfrBVU/weZP2gbPNRo9AAAAABJRU5ErkJggg==\n","text/plain":["<IPython.core.display.Image object>"]},"metadata":{}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACsklEQVR4nAXBPWwcVRAA4Jl5s29373yXO5/t2DqcsyByTLAQJPxECoSgRKZJQ4WEkjINCFFS0SNKSgqgo4EK0oRIiIICyRIKuCCIJLbkn9hOfOf17u3tvvdm+D5kYBd5E5REgdABoYUJECAwMyrVhjya2OZBzOq7+3/w0GdiOGDDB+MsemoWIfYUK59LjpKtQw4ozLUKMRAVEtUqDuzBnp9tPQNCpBKozS5UzmMcqbcQvQ5ufDbhEIhIQqXMEuTMNQeACj+VVjcDKgIBakvKWj2k95TAkVf02dNEVJVEcA3al7B9/u2XTgfw8z7CoNVJCeKJAV9ZuWnN5S/3hmoHz+/V0zFA5psoQg5mb30c66X541/+LXqvgdCNc4NOb+W7OSKM3fUfjzplMTw6xK+yXXmzffszST8P99fv1Zy6F/Pf1p4uxP2try8u//zGt2+9+snU350Ht66sP2Gne+vfuNEHO/uPd7tnF833S6PZ9x4+mrv98vI+Kzy7P7Px68Bdvnhh0X1qO9KPN//8wf6z/Vh4Ah8+2kieXJjLR53quBdvdJutONss04XtmhXyg+dG0HK5nxSNcXuh+k/Pbx5D/Tsis//rTltb7E+L0rhqdIM5lJWHgBCQgvF1PyxxldfjcniYYdro0xokMSERcVZcGY0T04qYeotJBFFCV6ESUSIfxL6TegEBBohNo4FozqAhZCVEad6174dxPS4NEUswpLGFiEQJYbu4E64LRGJQTwBAAMyyFUdAkO6Ty+cjR7FlA7UB7xGmZxRiQ9AoTTHV1WOX16REQBayMp8ZJKc84Wy//cJVT0vabQYwRrGaRPYj0yFS5l4IC9Ni0oN2EDflIJg6MYOdQk4Y0Fprrw1ddTJxwXkRUXHlFwmTQbQSQM3q0vLqOM0ebPVlZ5Hv7oYK0IT/AQtYZAnbBWHoAAAAAElFTkSuQmCC\n","text/plain":["<IPython.core.display.Image object>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"0JCpRuZ_wmoD"},"execution_count":null,"outputs":[]}]}